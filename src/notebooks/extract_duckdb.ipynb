{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import duckdb\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Setting pandas to display all columns and rows\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bca2413f10b586fc8e29e6a7712258baf2d528cfdfc06dcba5bf1cc44e714a9c\n"
     ]
    }
   ],
   "source": [
    "# Load API key from .env file\n",
    "load_dotenv('.env')\n",
    "api_key = os.getenv('API_KEY')\n",
    "print(api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API key from .env file\n",
    "api_key = os.getenv('API_KEY')\n",
    "\n",
    "# Create function to call API\n",
    "def extract(api_key):\n",
    "    \"\"\"\n",
    "    Fetch and process air quality measurements from the OpenAQ API.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    api_key : str\n",
    "        API key for authentication with the OpenAQ API.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pandas.DataFrame\n",
    "        Dataframe containing the fetched measurements.\n",
    "        \n",
    "    Raises\n",
    "    ------\n",
    "    requests.exceptions.HTTPError\n",
    "        For HTTP related errors.\n",
    "    requests.exceptions.ConnectionError\n",
    "        For connection related errors.\n",
    "    requests.exceptions.Timeout\n",
    "        For timeout errors.\n",
    "    requests.exceptions.RequestException\n",
    "        For all other request related errors.\n",
    "    Exception\n",
    "        If the fetched dataframe is empty.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # API URL\n",
    "    api_url = \"https://api.openaq.org/v2/measurements\"\n",
    "    \n",
    "    # Define the query parameters to API\n",
    "    params = {\n",
    "        \"location_id\": \"380422\",\n",
    "        \"parameter\": [\"pressure\", \"temperature\", \"um003\", \"um025\", \"um010\", \"pm10\", \"um100\", \"pm1\", \"um005\", \"humidity\", \"um050\", \"pm25\"],\n",
    "        \"limit\": 1000,\n",
    "        \"api_key\": api_key\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Make the GET request\n",
    "        response = requests.get(api_url, params=params, timeout=30)\n",
    "        \n",
    "        # Raise exception for HTTP errors\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            output = pd.json_normalize(data['results'])\n",
    "            df = pd.DataFrame(output)\n",
    "            \n",
    "            # Check if dataframe is empty\n",
    "            if df.empty:\n",
    "                raise Exception(\"Empty df, check API request\")\n",
    "            \n",
    "            df['date.utc'] = pd.to_datetime(df['date.utc'], errors='coerce') # Convert to datetime\n",
    "            df['date.local'] = df['date.utc'].dt.tz_convert('America/Los_Angeles') # Covert to PST/PDT time zone\n",
    "            df['date.local'] = df['date.local'].dt.tz_localize(None) # Convert to timezone-naive\n",
    "            df = df[df['value'] > 0.0] # Filter values\n",
    "            \n",
    "            return df\n",
    "        \n",
    "    except requests.exceptions.HTTPError as errh:\n",
    "        print(\"Http Error:\", errh)\n",
    "    except requests.exceptions.ConnectionError as errc:\n",
    "        print(\"Error Connecting:\", errc)\n",
    "    except requests.exceptions.Timeout as errt:\n",
    "        print(\"Timeout Error:\", errt)\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        print(\"Error: Something Else\", err)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 879 entries, 0 to 999\n",
      "Data columns (total 15 columns):\n",
      " #   Column                 Non-Null Count  Dtype              \n",
      "---  ------                 --------------  -----              \n",
      " 0   locationId             879 non-null    int64              \n",
      " 1   location               879 non-null    object             \n",
      " 2   parameter              879 non-null    object             \n",
      " 3   value                  879 non-null    float64            \n",
      " 4   unit                   879 non-null    object             \n",
      " 5   country                879 non-null    object             \n",
      " 6   city                   0 non-null      object             \n",
      " 7   isMobile               879 non-null    bool               \n",
      " 8   isAnalysis             0 non-null      object             \n",
      " 9   entity                 879 non-null    object             \n",
      " 10  sensorType             879 non-null    object             \n",
      " 11  date.utc               879 non-null    datetime64[ns, UTC]\n",
      " 12  date.local             879 non-null    datetime64[ns]     \n",
      " 13  coordinates.latitude   879 non-null    float64            \n",
      " 14  coordinates.longitude  879 non-null    float64            \n",
      "dtypes: bool(1), datetime64[ns, UTC](1), datetime64[ns](1), float64(3), int64(1), object(8)\n",
      "memory usage: 103.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# Call extract function\n",
    "df = extract(api_key)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ale\\Documents\\Data-Science-Projects\\Ploomber-2023\\ETL-pipeline-ML\\src\\notebooks\n",
      "c:\\Users\\Ale\\Documents\\Data-Science-Projects\\Ploomber-2023\\ETL-pipeline-ML\\src\n",
      "c:\\Users\\Ale\\Documents\\Data-Science-Projects\\Ploomber-2023\\ETL-pipeline-ML\\src\\data\n"
     ]
    }
   ],
   "source": [
    "# Define current working directory\n",
    "current_dir = os.getcwd()\n",
    "print(current_dir)\n",
    "\n",
    "# Get parent directory\n",
    "parent_directory = os.path.dirname(current_dir)\n",
    "print(parent_directory)\n",
    "\n",
    "# Define directory to store DuckDB database\n",
    "database_directory = os.path.join(parent_directory, \"data\")\n",
    "print(database_directory)\n",
    "\n",
    "# # Create data directory to store DuckDB database\n",
    "# Path(database_directory).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define table name\n",
    "table_name = \"air_data\"\n",
    "\n",
    "# Create local DuckDB instance\n",
    "def init_duckdb(df, table_name, database_directory):\n",
    "    \"\"\"\n",
    "    Initiate a DuckDB instance to create a DuckDB database named \"air_data.duckdb\" inside the\n",
    "    given `database_directory` if it does not already exist. If the specified `table_name`\n",
    "    does not exist in the database, it will be created and the given DataFrame `df`\n",
    "    will be stored in it.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The DataFrame to be registered and stored in the DuckDB database.\n",
    "    table_name : str\n",
    "        Name of the table in which the DataFrame should be stored.\n",
    "    database_directory : str\n",
    "        Directory path where the DuckDB database file (\"air_data.duckdb\") will be located.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "\n",
    "    \"\"\"\n",
    "    duckdb_directory = os.path.join(database_directory, \"air_data.duckdb\")\n",
    "    con = duckdb.connect(duckdb_directory)\n",
    "    con.register('df', df)\n",
    "    \n",
    "    # Check if table already exists, if not, create it\n",
    "    tables = con.execute(\"SHOW TABLES\").fetchall()\n",
    "    if table_name not in [table[0] for table in tables]:\n",
    "        con.execute(f\"CREATE TABLE {table_name} AS SELECT * FROM df\")\n",
    "    \n",
    "    con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call init_duckdb function\n",
    "init_duckdb(df, table_name, database_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get MotherDuck token\n",
    "md_token = os.getenv('MOTHERDUCK_TOKEN')\n",
    "\n",
    "# Check if MotherDuck token is set into the environment\n",
    "assert md_token and md_token.strip() != '', \"MOTHERDUCK_TOKEN environment variable is not set or is empty.\"\n",
    "\n",
    "# # Define database directory\n",
    "# duckdb_directory = os.path.join(database_directory, \"air_data.duckdb\")\n",
    "\n",
    "# Initiate the MotherDuck connection with token\n",
    "local_con = duckdb.connect(f'md:?motherduck_token={md_token}')\n",
    "\n",
    "# Load MotherDuck extension\n",
    "local_con.execute(\"LOAD motherduck\")\n",
    "\n",
    "# Load air_data.duckdb database into MotherDuck database named openaq_api\n",
    "local_con.execute(\"CREATE OR REPLACE TABLE openaq_api.main.df as SELECT * FROM 'df'\")\n",
    "\n",
    "local_con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionException",
     "evalue": "Connection Error: Connection has already been closed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionException\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Ale\\Documents\\Data-Science-Projects\\Ploomber-2023\\ETL-pipeline-ML\\src\\notebooks\\extract_duckdb.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Ale/Documents/Data-Science-Projects/Ploomber-2023/ETL-pipeline-ML/src/notebooks/extract_duckdb.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m air_df \u001b[39m=\u001b[39m local_con\u001b[39m.\u001b[39;49msql(\u001b[39m\"\u001b[39;49m\u001b[39mSELECT * FROM df\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39mdf()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ale/Documents/Data-Science-Projects/Ploomber-2023/ETL-pipeline-ML/src/notebooks/extract_duckdb.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m air_df\u001b[39m.\u001b[39mhead()\n",
      "\u001b[1;31mConnectionException\u001b[0m: Connection Error: Connection has already been closed"
     ]
    }
   ],
   "source": [
    "# air_df = local_con.sql(\"SELECT * FROM df\").df()\n",
    "# air_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duckdb_df = duckdb.sql(\"SELECT * FROM df\").df()\n",
    "# duckdb_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ale\\Documents\\Data-Science-Projects\\Ploomber-2023\\ETL-pipeline-ML\\src\\notebooks\n",
      "c:\\Users\\Ale\\Documents\\Data-Science-Projects\\Ploomber-2023\\ETL-pipeline-ML\\src\n",
      "c:\\Users\\Ale\\Documents\\Data-Science-Projects\\Ploomber-2023\\ETL-pipeline-ML\\src\\data\\air_data.duckdb\n"
     ]
    }
   ],
   "source": [
    "# # Code to call database into a pandas dataframe\n",
    "\n",
    "# import duckdb\n",
    "# import os\n",
    "# from pathlib import Path\n",
    "\n",
    "# current_dir = os.getcwd()\n",
    "# print(current_dir)\n",
    "\n",
    "# parent_directory = os.path.dirname(current_dir)\n",
    "# print(parent_directory)\n",
    "\n",
    "# database_directory = os.path.join(parent_directory, \"data\", \"air_data.duckdb\")\n",
    "# print(database_directory)\n",
    "\n",
    "\n",
    "# con = duckdb.connect(database_directory)\n",
    "\n",
    "# result = con.execute(\"SELECT * FROM air_data;\")\n",
    "\n",
    "# df = result.fetch_df()\n",
    "# df.head()\n",
    "\n",
    "# con.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automate-etl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
